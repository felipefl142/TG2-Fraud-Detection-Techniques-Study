{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='background:#3E4491; border:2; border-radius:5px; font-size:200%; font-weight: bold;\n",
    "color:white; padding:20px'><center>Estudo TG2 - Técnicas de Detecção de Fraudes usando Sistemas Inteligentes Híbridos</center></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dc0069e",
   "metadata": {},
   "source": [
    "<center><img src = \"Fraud_Detect.jpg\" width = 900 height = 400/></center>\n",
    "\n",
    "* Dataset (https://www.kaggle.com/datasets/chitwanmanchanda/fraudulent-transactions-data):\n",
    "\n",
    "    Dados fictícios que simulam 6.5 milhões de transações financeiras em sistema de carteira digital mobile, em que ~0,13% são fraudes. Trata-se de problema de aprendizado supervisionado (labeled data). Dados gerados pelo software Paysim, criado por Edgar Lopez e encontrado em https://github.com/EdgarLopezPhD/PaySim\n",
    "\n",
    "* Tarefa:\n",
    "Estudar técnicas diversas de aprendizado de máquina para detecção de fraudes financeiras, com foco Stacking Ensembles.\n",
    "\n",
    "<a id='top'></a>\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "\n",
    "<h1 style='background:#3E4491; border:0; border-radius: 10px; color:white;padding:20px'><center> Sumário </center></h1>\n",
    "\n",
    "### [**1. Importando bibliotecas, carregando dados e Data Wrangling**](#title-one)\n",
    "\n",
    "### [**2. EDA e Seleção de Features**](#title-two)\n",
    "\n",
    "### [**3. Limpeza de dados**](#title-three)\n",
    "\n",
    "### [**4. Pré-processamento de Dados**](#title-four)\n",
    "\n",
    "### [**5. Modelagem**](#title-five)\n",
    "\n",
    "### [**6. Pickle e Simulação de Deploy**](#title-six)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title-one\"></a>\n",
    "<h1 style='background:#3E4491; border:2; border-radius: 10px; color:white;padding:20px'><center>Importando bibliotecas, carregando dados e Data Wrangling</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "sns.color_palette('hls', 8)\n",
    "\n",
    "# Definindo fonte\n",
    "font = {'family': 'DejaVu Sans',\n",
    "        'weight': 'bold',\n",
    "        'size': 30\n",
    "        }\n",
    "\n",
    "# Ignorando warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = pd.read_csv('Fraud.csv')\n",
    "fraud_df_orig = fraud_df.copy()\n",
    "\n",
    "fraud_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df.sample(10, random_state = 51)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Colunas:\n",
    "\n",
    "* step - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).\n",
    "\n",
    "* type - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\n",
    "\n",
    "* amount - amount of the transaction in local currency.\n",
    "\n",
    "* nameOrig - customer who started the transaction\n",
    "\n",
    "* oldbalanceOrg - initial balance before the transaction\n",
    "\n",
    "* newbalanceOrig - new balance after the transaction\n",
    "\n",
    "* nameDest - customer who is the recipient of the transaction\n",
    "\n",
    "* oldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).\n",
    "\n",
    "* newbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).\n",
    "\n",
    "* isFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.\n",
    "\n",
    "* isFlaggedFraud - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceando o Dataset (Undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frauds = fraud_df[fraud_df['isFraud'] == 1]\n",
    "legits = fraud_df[fraud_df['isFraud'] == 0]\n",
    "\n",
    "n_legits = len(legits)\n",
    "n_frauds = len(frauds)\n",
    "\n",
    "legits_percent = (n_legits / (n_frauds + n_legits)) * 100\n",
    "frauds_percent = (n_frauds / (n_frauds + n_legits)) * 100\n",
    "\n",
    "print(\"Número de transações legítimas: \", n_legits, '({:.4f}%).'.format(legits_percent))\n",
    "print(\"Número de transações fraudulentas: \", n_frauds, '({:.4f}%).'.format(frauds_percent))\n",
    "\n",
    "#Pegando 80% do total de transações fraudulentas para melhorar a performance do modelo de deploy\n",
    "frauds = fraud_df[fraud_df['isFraud'] == 1].sample(frac = 0.82, random_state = 42)\n",
    "n_frauds = len(frauds) #resetando valor de n_frauds para a mudança anterior\n",
    "\n",
    "legits = legits.sample(n = n_frauds, random_state = 42)\n",
    "df_reduced = pd.concat([frauds, legits], axis = 0)\n",
    "\n",
    "fraud_value_counts = df_reduced.isFraud.value_counts(normalize = True)\n",
    "print(f'\\nFazendo o Undersampling, temos agora {fraud_value_counts[0]*100}% de transações legítimas e {fraud_value_counts[1]*100}% de transações fraudulentas.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title-two\"></a>\n",
    "<h1 style='background:#3E4491; border:2; border-radius: 10px; color:white;padding:20px'><center>EDA e Seleção de Features</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lista das variáveis numéricas de ponto flutuante\n",
    "numericals = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "df_reduced[numericals].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando biblioteca p/ calcular o VIF (Variance Inflation Factor)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(df):\n",
    "\n",
    "    # Calculando VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = df.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "\n",
    "    return(vif)\n",
    "\n",
    "calc_vif(df_reduced.drop(['step', 'type', 'nameOrig', 'nameDest'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Podemos ver que Saldo Anterior na Origem (*oldbalanceOrg*) e Saldo Atual na Origem (*newbalanceOrig*) tem altíssimos VIF's, assim mostrando que estão altamente correlacionados.\n",
    "# # O mesmo ocorre para Saldo Anterior no Destino (*oldbalanceDest*) e Saldo Atual no Destino (*newbalanceDest*). Vamos juntar esse par de variáveis colineares e *dropar* elas individualmente.\n",
    "\n",
    "# df_reduced['actualAmountOrig'] = df_reduced.apply(lambda x: x['oldbalanceOrg'] - x['newbalanceOrig'], axis = 1)\n",
    "# df_reduced['actualAmountDest'] = df_reduced.apply(lambda x: x['oldbalanceDest'] - x['newbalanceDest'], axis = 1)\n",
    "\n",
    "# # Dropando colunas individuais\n",
    "# df_reduced = df_reduced.drop(['oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest'],axis = 1)\n",
    "\n",
    "# # Recalculando VIF\n",
    "# calc_vif(df_reduced.drop(['step', 'type', 'nameOrig', 'nameDest'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,18))\n",
    "\n",
    "corr = df_reduced.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap='PuOr', vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
    "            linecolor='k', fmt='.2f', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_colunas = {'amount': 'Montante',\n",
    "                'oldbalanceOrg': 'Saldo Anterior - Origem',\n",
    "                'newbalanceOrig': 'Saldo Atual - Origem',\n",
    "                'oldbalanceDest': 'Saldo Anterior - Destino',\n",
    "                'newbalanceDest': 'Saldo Atual - Destino',\n",
    "                'step': 'Passo - Tempo',\n",
    "                'isFraud': 'Fraude',\n",
    "                'isFlaggedFraud': 'Flag de Fraude',\n",
    "                }\n",
    "\n",
    "fig = plt.figure(figsize= (15,9))\n",
    "\n",
    "num_cols = list(df_reduced.select_dtypes(exclude=['object','int64']).columns)\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax=fig.add_subplot(len(nome_colunas)-3,1,i+1)\n",
    "    sns.boxplot(x=df_reduced[col], ax=ax)\n",
    "    plt.xlabel(nome_colunas[col], fontdict={'fontsize': 15})\n",
    "    fig.tight_layout()\n",
    "\n",
    "fig.suptitle('Distribuição de variáveis - Sem limpeza de Intervalo Interquartil\\n', size = 24)\n",
    "plt.subplots_adjust(top=0.90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(nome_colunas)-3, 1, figsize = (12,18))\n",
    "\n",
    "for i, col in enumerate(numericals):\n",
    "    sns.histplot(data = df_reduced, x = col, hue = 'isFraud', ax = axs[i], bins=40)\n",
    "    axs[i].set_xlabel(nome_colunas[col], fontdict={'fontsize': 15})\n",
    "    axs[i].set_ylabel('Frequência', fontdict={'fontsize': 15})\n",
    "    axs[i].legend(['Fraude', 'Legítima'])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "fig.suptitle('Histograma das variáveis numéricas - Antes da limpeza', fontsize=29);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bokeh.plotting import figure, output_notebook, show, output_file\n",
    "# from bokeh.models import ColumnDataSource\n",
    "\n",
    "# p = figure(plot_width = 300, plot_height = 300, tools = \"pan,reset,save\")\n",
    "# source = ColumnDataSource(data = dict(x = sample1.amount, y = sample1.index))\n",
    "\n",
    "# p.circle(x='x', y = 'y', source=source, color = 'navy', alpha = 0.5)\n",
    "\n",
    "# output_notebook()\n",
    "# show(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As features selecionadas são: Montante, Saldo Anterior - Origem, Saldo Atual - Origem, Saldo Anterior - Destino, Saldo Atual - Destino e **Fraude (variável alvo)**. As variáveis *Step*, nome das contas e *flag* de fraude foram *dropadas* pois não representavam nenhum poder preditivo e poderiam atrapalhar o treinamento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title-three\"></a>\n",
    "<h1 style='background:#3E4491; border:2; border-radius: 10px; color:white;padding:20px'><center>Limpeza de Dados</center></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataprep.eda import plot\n",
    "\n",
    "# plot(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_reduced.copy()\n",
    "\n",
    "print('Número de linhas pré limpeza: ', df_reduced.shape[0])\n",
    "\n",
    "# Limpando as colunas numéricas (fazendo somente limite superior pois muitos valores nulos - contas sem dinheiro)\n",
    "\n",
    "for i in ['amount', 'oldbalanceOrg', 'oldbalanceDest', 'newbalanceDest']: #numericals dá erro porque newbalanceOrig tem praticamente todos seus valores nulos assim dropnando todo o dataset\n",
    "    \n",
    "    q1, q3 = np.percentile(df_clean[i], [25, 75])\n",
    "    \n",
    "    iqr = q3 - q1\n",
    "    lim_sup = q3 + 1.75 * iqr\n",
    "\n",
    "    indice = df_clean[df_clean[i] >= lim_sup].index\n",
    "\n",
    "    df_clean.drop(indice, inplace= True)\n",
    "\n",
    "print('Número de linhas pós limpeza: ', df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= (15,9))\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax=fig.add_subplot(len(nome_colunas)-3,1,i+1)      \n",
    "    sns.boxplot(x = df_clean[col], ax=ax)\n",
    "    plt.xlabel(nome_colunas[col], fontdict={'fontsize': 15})\n",
    "    fig.tight_layout()\n",
    "\n",
    "fig.suptitle('Distribuição de variáveis - Após limpeza de Intervalo Interquartil\\n', size = 24)\n",
    "plt.subplots_adjust(top=0.90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(nome_colunas)-3, 1, figsize = (12,18))\n",
    "\n",
    "for i, col in enumerate(numericals):\n",
    "    sns.histplot(data = df_clean, x = col, hue = 'isFraud', ax = axs[i], bins=40)\n",
    "    axs[i].set_xlabel(nome_colunas[col], fontdict={'fontsize': 15})\n",
    "    axs[i].set_ylabel('Frequência', fontdict={'fontsize': 15})\n",
    "    axs[i].legend(['Fraude', 'Legítima'])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "fig.suptitle('Histograma das variáveis numéricas - Após limpeza', fontsize=29);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title-four\"></a>\n",
    "<h1 style='background:#3E4491; border:2; border-radius: 10px; color:white;padding:20px'><center>Pré-processamento de Dados</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports de Modelagem (Pré-processamento: Normalização, Splits; Validação; e Modelos)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report, roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Modelos\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, StackingClassifier, IsolationForest\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalização das variáveis numéricas\n",
    "\n",
    "df_clean[numericals] = StandardScaler().fit_transform(df_clean[numericals])\n",
    "\n",
    "# Codificação da coluna 'Type' (categórica)\n",
    "\n",
    "codec = pd.DataFrame(df_reduced['type'].factorize()[1], columns = ['type'])\n",
    "print('* Codificação da variável categórica \"type\":  \\n\\n', codec)\n",
    "\n",
    "df_clean['type'] = df_clean['type'].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição de Features e Target\n",
    "\n",
    "features = ['type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "X = df_clean[features]\n",
    "y = df_clean['isFraud']\n",
    "\n",
    "#Splits\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title-five\"></a>\n",
    "<h1 style='background:#3E4491; border:2; border-radius: 10px; color:white;padding:20px'><center>Modelagem</center></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando Modelos com cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando comparação de acurácia entre os 6 modelos escolhidos\n",
    "\n",
    "comparacao_modelos = pd.DataFrame(columns = ['Modelo', 'Score'])\n",
    "\n",
    "modelos = [LogisticRegression('l2', solver='lbfgs', random_state=42),\n",
    "           KNeighborsClassifier(n_neighbors = 41, p = 4),\n",
    "           ExtraTreesClassifier(random_state = 42),\n",
    "           IsolationForest(n_jobs = -1, random_state = 42),\n",
    "           XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1),\n",
    "           StackingClassifier(estimators = [('xgbc', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)),\n",
    "                                           ('log_reg', LogisticRegression(n_jobs = -1, random_state=42))\n",
    "                                           ],\n",
    "                              final_estimator=LogisticRegression('l2', solver='lbfgs', random_state=42)\n",
    "                              )\n",
    "           ]\n",
    "\n",
    "for model in modelos:\n",
    "    model_name = model.__class__.__name__\n",
    "    scores = cross_val_score(model, X_train, y_train, cv = 5, scoring = 'accuracy')\n",
    "    comparacao_modelos.loc[len(comparacao_modelos)] = [model_name, scores.mean().round(4)]\n",
    "\n",
    "comparacao_modelos = comparacao_modelos.sort_values(by='Score', ascending=False)\n",
    "\n",
    "print('* Comparação Cross-Validation Score entre modelos: ')\n",
    "comparacao_modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "519b6fb6",
   "metadata": {},
   "source": [
    "### * Modelo Baseline: Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49837670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo objeto do modelo e seus parâmetros\n",
    "\n",
    "dummymodel = DummyClassifier(strategy = 'prior')\n",
    "dummymodel.fit(X_train, y_train) # Treinando modelo\n",
    "\n",
    "y_pred_dummy = dummymodel.predict(X_test) # Criando lista das predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5025ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas e Matriz de Confusão\n",
    "\n",
    "print('Acurácia: ', accuracy_score(y_test, y_pred_dummy).round(4))\n",
    "\n",
    "sns.set_theme(style = 'white')\n",
    "cmd1 = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_dummy), display_labels = ['Normal', 'Fraude'])\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "plt.grid(False)\n",
    "cmd1.plot(ax = ax)\n",
    "plt.rc('font', **font)\n",
    "plt.title('Matriz de confusão - Dummy Classifier', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 1º Modelo: Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo objeto do modelo e seus parâmetros constantes\n",
    "\n",
    "xtc = ExtraTreesClassifier(warm_start = True, n_jobs = -1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo espaço de busca dos hiperparâmetros e executando otimização Bayesiana destes\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "xtc_param_dict = {'n_estimators': Integer(2, 800),\n",
    "                  'criterion': Categorical(['gini', 'entropy']),\n",
    "                  'max_depth': Integer(1, 15)\n",
    "                  }\n",
    "\n",
    "bayessearch_xtc = BayesSearchCV(xtc,\n",
    "                                search_spaces = xtc_param_dict,\n",
    "                                scoring = 'f1',\n",
    "                                cv = 3,\n",
    "                                refit = 'recall',\n",
    "                                # verbose = 1,\n",
    "                                n_jobs = -1,\n",
    "                                random_state = 42\n",
    "                                ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo resultados: Melhores parâmetros, melhor score, report de métricas e matriz confusão\n",
    "\n",
    "print('\\nMelhores Parâmetros: ', bayessearch_xtc.best_params_,\n",
    "      '\\nMelhor Score: ', bayessearch_xtc.best_score_,\n",
    "      '\\n', classification_report(y_test, bayessearch_xtc.predict(X_test))\n",
    "      )\n",
    "\n",
    "xtc_cm = ConfusionMatrixDisplay(confusion_matrix(y_test, bayessearch_xtc.predict(X_test)),\n",
    "                                display_labels = ['Normal', 'Fraude']\n",
    "                                ).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance (a título de curiosidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para renomear as features na visualização\n",
    "feats_ptbr = ['Tipo de Transação',\n",
    "              'Montante',\n",
    "              'Saldo Anterior - Origem',\n",
    "              'Saldo Atual - Origem',\n",
    "              'Saldo Anterior - Destino',\n",
    "              'Saldo Atual - Destino'\n",
    "              ]\n",
    "\n",
    "# Chamando método de importâncias das features para o modelo ExtraTreesClassifier\n",
    "feat_importance = pd.Series(xtc.fit(X_train, y_train).feature_importances_,\n",
    "                            index=feats_ptbr\n",
    "                            ).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "feat_importance.plot(kind = 'bar', color = '#3E4491')\n",
    "plt.grid(which = 'major', axis = 'y', linestyle = '--')\n",
    "plt.xticks(fontsize = 19);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 2º Modelo: XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo objeto do modelo e seus parâmetros constantes\n",
    "\n",
    "xgbc = XGBClassifier(eval_metric = 'logloss',\n",
    "                     objective = 'binary:logistic',\n",
    "                     use_label_encoder = False,\n",
    "                     n_jobs = -1\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo espaço de busca dos hiperparâmetros e executando otimização Bayesiana destes\n",
    "\n",
    "xgbc_param_dict = {'learning_rate': Real(0.01, 0.99),\n",
    "                   'max_depth': Integer(3,8),\n",
    "                   'max_leaves': Integer(10, 100),\n",
    "                  'n_estimators': Integer(1, 1000)\n",
    "                   }\n",
    "\n",
    "bayessearch_xgbc = BayesSearchCV(xgbc,\n",
    "                                 search_spaces = xgbc_param_dict,\n",
    "                                 scoring = 'recall',\n",
    "                                 cv = 3,\n",
    "                                 refit = 'recall',\n",
    "                                #  verbose = 1,\n",
    "                                 n_jobs = -1,\n",
    "                                 random_state = 42\n",
    "                                 ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo resultados: Melhores parâmetros, melhor score, report de métricas e matriz confusão\n",
    "\n",
    "print('\\nMelhores Parâmetros: ', bayessearch_xgbc.best_params_,\n",
    "      '\\n Melhor Score: ', bayessearch_xgbc.best_score_, '\\n',\n",
    "      classification_report(y_test, bayessearch_xgbc.predict(X_test))\n",
    "      )\n",
    "xgbc_cm = ConfusionMatrixDisplay(confusion_matrix(y_test, bayessearch_xgbc.predict(X_test)), display_labels = ['Normal', 'Fraude']).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 3º Modelo: Isolation Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo objeto do modelo e seus parâmetros constantes\n",
    "\n",
    "ifc = IsolationForest(n_estimators = 7,\n",
    "                      max_samples = 0.75,\n",
    "                      contamination = 0.5,\n",
    "                      max_features = 1,\n",
    "                      n_jobs = -1,\n",
    "                      warm_start = True,\n",
    "                      random_state = 42\n",
    "                      ).fit(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gambiarra para converter o resultado do modelo para 0 e 1, como a saída do predict dos outros modelos\n",
    "# mapeando -1 para 1 e 1 para 0, método numpy\n",
    "\n",
    "numpy_map = np.vectorize(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "ifc_y_pred = numpy_map(ifc.predict(X_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo resultados: report de métricas e matriz confusão\n",
    "\n",
    "print(classification_report(y_test, ifc_y_pred))\n",
    "ifc_cm = ConfusionMatrixDisplay(confusion_matrix(y_test, ifc_y_pred), display_labels = ['Normal', 'Fraude']).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa de fazer Isolation Forest funcionar como preditor padrão e otimizar via Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ifc_param_dict = {'n_estimators': Integer(2, 1000),\n",
    "#                   'bootstrap': Categorical([True, False]),\n",
    "#                   'max_features': Integer(1, 6)\n",
    "#                   }\n",
    "\n",
    "# bayessearch_ifc = BayesSearchCV(ifc,\n",
    "#                                 search_spaces = ifc_param_dict,\n",
    "#                                 scoring = 'recall',\n",
    "#                                 cv = 3,\n",
    "#                                 refit = 'recall',\n",
    "#                                 verbose = 1,\n",
    "#                                 n_jobs = -1,\n",
    "#                                 random_state = 42\n",
    "#                                 ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ifc = fraud_df.copy()\n",
    "\n",
    "# df_ifc = df_ifc.drop(['step', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)\n",
    "\n",
    "# df_ifc[numericals] = StandardScaler().fit_transform(df_ifc[numericals])\n",
    "# df_ifc['type'] = df_ifc['type'].factorize()[0]\n",
    "\n",
    "# X_train_ifc, X_test_ifc, y_train_ifc, y_test_ifc = train_test_split(df_ifc.drop('isFraud', axis=1), df_ifc['isFraud'], test_size=0.3, stratify=df_ifc['isFraud'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Isolation Forest\n",
    "\n",
    "# ifc_unbalance = IsolationForest(n_estimators = 100,\n",
    "#                                 max_samples = 0.4,\n",
    "#                                 contamination = df_ifc['isFraud'].value_counts(normalize=True)[1]*10,\n",
    "#                                 max_features = 1,\n",
    "#                                 n_jobs = -1,\n",
    "#                                 random_state = 42\n",
    "#                                 ).fit(X_train_ifc.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy_map = np.vectorize(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "# ifc_y_pred_unbalance = numpy_map(ifc.predict(X_test_ifc.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_ifc['isFraud'].value_counts(normalize=True)[1])\n",
    "\n",
    "# print(pd.DataFrame(ifc.predict(X_test_ifc.values)).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test_ifc, ifc_y_pred_unbalance))\n",
    "# ifc_cm = ConfusionMatrixDisplay(confusion_matrix(y_test_ifc, ifc_y_pred_unbalance), display_labels=['Normal', 'Fraude']).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados muito ruins - mesmo utilizando de todos os 6.5M de dados para treino/teste, fazendo gambiarras para usar otimização bayesiana e como preditor comum, e ainda assim - , modelo pior que o mero chute! Não vale e pena seguir com esse modelo para o Stacking Ensemble. Utilizaremos a Regressão Logística, já que se trata de modelo com relação custo (computacional)-benefício (resultados, métricas etc) das mais altas na classificação, fora ainda a interpretabilidade e simplicidade de explicação."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 4º Modelo: Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo objeto do modelo e seus parâmetros constantes\n",
    "\n",
    "log_reg = LogisticRegression(n_jobs = -1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo espaço de busca dos hiperparâmetros e executando otimização Bayesiana destes\n",
    "\n",
    "logreg_param_dict = {'penalty': Categorical(['l2', 'none']),\n",
    "                     'C': Real(0.01, 1.0),\n",
    "                     'class_weight': Categorical(['balanced', None]),\n",
    "                     'solver': Categorical(['newton-cg', 'lbfgs', 'sag', 'saga'])\n",
    "                     }\n",
    "\n",
    "bayessearch_logreg = BayesSearchCV(log_reg,\n",
    "                                   search_spaces = logreg_param_dict,\n",
    "                                   scoring = 'recall',\n",
    "                                   cv = 3,\n",
    "                                   refit = 'recall',\n",
    "                                #    verbose = 1,\n",
    "                                   n_jobs = -1,\n",
    "                                   random_state = 42\n",
    "                                   ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo resultados: Melhores parâmetros, melhor score, report de métricas e matriz confusão\n",
    "\n",
    "print('\\nMelhores Parâmetros: ', bayessearch_logreg.best_params_,\n",
    "      '\\n Melhor Score: ', bayessearch_logreg.best_score_, '\\n',\n",
    "      classification_report(y_test, bayessearch_logreg.predict(X_test)))\n",
    "\n",
    "logreg_cm = ConfusionMatrixDisplay(confusion_matrix(y_test, bayessearch_logreg.predict(X_test)), display_labels = ['Normal', 'Fraude']).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 5º Modelo: Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo objeto do modelo: modelos base, modelo final e seus respectivos parâmetros constantes\n",
    "\n",
    "stack = StackingClassifier(estimators = [('xgbc', bayessearch_xgbc.best_estimator_),\n",
    "                                         ('log_reg', bayessearch_logreg.best_estimator_)\n",
    "                                         ],\n",
    "                           final_estimator = LogisticRegression(solver = 'lbfgs', class_weight = 'balanced', random_state = 41, n_jobs = -1, max_iter = 500),\n",
    "                           n_jobs = -1,\n",
    "                           passthrough = True\n",
    "                           ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo resultados: Acurácia, report de métricas e matriz confusão\n",
    "\n",
    "print('Acurácia: ', accuracy_score(y_test, stack.predict(X_test)).round(4))\n",
    "print(classification_report(y_test, stack.predict(X_test)))\n",
    "stack_cm = ConfusionMatrixDisplay(confusion_matrix(y_test, stack.predict(X_test)), display_labels = ['Normal', 'Fraude']).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativo Final dos Modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabela de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparativo_final_metricas = pd.DataFrame(columns = ['Acurácia', 'Recall', 'FN', 'F1-Score', 'AUROC'])\n",
    "\n",
    "comparativo_dict = {'Dummy': dummymodel.predict(X_test),\n",
    "                    'Extra Trees': bayessearch_xtc.predict(X_test),\n",
    "                    'XGBoost': bayessearch_xgbc.predict(X_test),\n",
    "                    'Regressão Logística': bayessearch_logreg.predict(X_test),\n",
    "                    'Stacking': stack.predict(X_test),\n",
    "                    'Isolation Forest': ifc_y_pred\n",
    "                    }\n",
    "\n",
    "for name in comparativo_dict:\n",
    "    comparativo_final_metricas.loc[name] = [accuracy_score(y_test, comparativo_dict[name]).round(4),\n",
    "                                            recall_score(y_test, comparativo_dict[name]).round(4),\n",
    "                                            confusion_matrix(y_test, comparativo_dict[name])[1][0],\n",
    "                                            f1_score(y_test, comparativo_dict[name]).round(4),\n",
    "                                            roc_auc_score(y_test, comparativo_dict[name]).round(4)\n",
    "                                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa51458",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparativo_final_metricas = comparativo_final_metricas.sort_values(by = 'Recall', ascending = False)\n",
    "comparativo_final_metricas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de curvas ROC e AUC para cada modelo\n",
    "\n",
    "d_auc = roc_auc_score(y_test, dummymodel.predict(X_test)) #Dummy AUC\n",
    "p_auc = roc_auc_score(y_test, y_test) #Perfection AUC\n",
    "xtc_auc = roc_auc_score(y_test, bayessearch_xtc.predict(X_test)) #ExtraTrees AUC\n",
    "xgb_auc = roc_auc_score(y_test, bayessearch_xgbc.predict(X_test)) #XGBoost AUC\n",
    "ifc_auc = roc_auc_score(y_test, ifc_y_pred) #Isolation Forest AUC\n",
    "reglog_auc = roc_auc_score(y_test, bayessearch_logreg.predict(X_test)) #Regressão Logística AUC\n",
    "stack_auc = roc_auc_score(y_test, stack.predict(X_test)) #Stacking AUC\n",
    "\n",
    "d_fpr, d_tpr, _ = roc_curve(y_test, dummymodel.predict(X_test)) #Dummy\n",
    "p_fpr, p_tpr, _ = roc_curve(y_test, y_test) #Perfection\n",
    "xtc_fpr, xtc_tpr, _ = roc_curve(y_test, bayessearch_xtc.predict(X_test)) #ExtraTrees\n",
    "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, bayessearch_xgbc.predict(X_test)) #XGBoost\n",
    "ifc_fpr, ifc_tpr, _ = roc_curve(y_test, ifc_y_pred) #Isolation Forest\n",
    "reglof_fpr, reglof_tpr, _ = roc_curve(y_test, bayessearch_logreg.predict(X_test)) #Regressão Logística\n",
    "stack_fpr, stack_tpr, _ = roc_curve(y_test, stack.predict(X_test)) #Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar\n",
    "\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "plt.plot(d_fpr, d_tpr, linestyle = '--', label = 'Dummy Model (AUROC = %0.3f)' % d_auc)\n",
    "plt.plot(p_fpr, p_tpr, linestyle = '--', label = 'Perfect Predictions (AUROC = %0.3f)' % p_auc)\n",
    "plt.plot(xtc_fpr, xtc_tpr, marker = '.', label = 'Extra Trees (AUROC = %0.3f)' % xtc_auc)\n",
    "plt.plot(xgb_fpr, xgb_tpr, marker = '.', label = 'XGBoost (AUROC = %0.3f)' % xgb_auc)\n",
    "plt.plot(ifc_fpr, ifc_tpr, marker = '.', label = 'Isolation Forest (AUROC = %0.3f)' % ifc_auc)\n",
    "plt.plot(stack_fpr, stack_tpr, marker = '.', label = 'Stacking (AUROC = %0.3f)' % stack_auc)\n",
    "\n",
    "# Título\n",
    "plt.title('Curvas ROC - Modelos', fontsize = 30)\n",
    "\n",
    "# Rótulo dos eixos\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Legenda\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"title-six\"></a>\n",
    "<h1 style='background:#3E4491; border:2; border-radius: 10px; color:white;padding:20px'><center>Pickle e Simulação de Deploy</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f05dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando dataframe de deploy (cópia do dataset completo, com as transformações idênticas às de treino)\n",
    "\n",
    "df_all = fraud_df.copy()\n",
    "df_all = df_all.drop(['step', 'nameOrig', 'nameDest', 'isFlaggedFraud'],\n",
    "                     axis = 1\n",
    "                     )\n",
    "\n",
    "df_all = df_all.drop(df_clean.index, axis = 0)\n",
    "\n",
    "df_all[numericals] = StandardScaler().fit_transform(df_all[numericals])\n",
    "df_all['type'] = df_all['type'].factorize()[0]\n",
    "\n",
    "X_all = df_all.drop('isFraud', axis = 1)\n",
    "y_all = df_all['isFraud']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aproveitando que utilizamos apenas parte dos dados nos treinamentos (graças ao undersampling 99.867% dos dados ficaram de fora), podemos utilizar os dados que não foram vistos pelos modelos e simular o *deploy* do melhor modelo em dados 'novos', assim testando a capacidade de generalização do preditor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, primeiro vamos criar uma linearização exportável do modelo. Assim, numa situação comercial, teríamos um arquivo pronto para utilizar em servidor e/ou serviço de computação na nuvem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A título de exemplo e para análise comparativa, primeiro vamos fazer o deploy somente com o melhor modelo de Regressão Logística (**sozinha, SEM** o XGBoost do Stacking):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f837e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializando o melhor modelo de Regressão Logística\t\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(bayessearch_logreg.best_estimator_, open('modelo_logreg.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguinte célula representa tudo o que o servidor teria que realizar com dados novos para realizar as predições em campo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando modelo e fazendo predições\n",
    "\n",
    "pickle_model_logreg = pickle.load(open('modelo_logreg.pkl', 'rb'))\n",
    "\n",
    "y_pred_deploy_logreg = pickle_model_logreg.predict(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cdd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo resultados: Acurácia, report de métricas e matriz confusão\n",
    "\n",
    "print('Acurácia: ', accuracy_score(y_all, y_pred_deploy_logreg).round(4))\n",
    "\n",
    "print(classification_report(y_all, y_pred_deploy_logreg))\n",
    "\n",
    "cmp2 = ConfusionMatrixDisplay(confusion_matrix(y_all, y_pred_deploy_logreg), display_labels = ['Normal', 'Fraude'])\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "plt.grid(False)\n",
    "cmp2.plot(ax = ax)\n",
    "plt.rc('font', **font)\n",
    "plt.title('Matriz de confusão - Pickle Regressão Logística (Simulação Deploy)', fontsize = 14)\n",
    "plt.show()\n",
    "\n",
    "print('* Porcentagens da matriz de confusão:\\n')\n",
    "logreg_deploy_cm = (100 * confusion_matrix(y_all, y_pred_deploy_logreg, normalize='all'))\n",
    "for n, i in enumerate(logreg_deploy_cm):\n",
    "    logreg_deploy_cm[n] = i.astype('float').round(3)\n",
    "    \n",
    "print(logreg_deploy_cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Deploy com Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f837e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializando melhor modelo de Stacking (modelo final)\n",
    "\n",
    "pickle.dump(stack, open('modelo_final.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguinte célula representa tudo o que o servidor teria que realizar com dados novos para realizar as predições em campo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f05dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando modelo e fazendo predições\n",
    "\n",
    "pickle_model = pickle.load(open('modelo_final.pkl', 'rb'))\n",
    "\n",
    "y_pred_deploy = pickle_model.predict(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cdd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo resultados: Acurácia, report de métricas e matriz confusão\n",
    "\n",
    "print('Acurácia: ', accuracy_score(y_all, y_pred_deploy).round(4))\n",
    "\n",
    "print(classification_report(y_all, y_pred_deploy))\n",
    "cmp1 = ConfusionMatrixDisplay(confusion_matrix(y_all, y_pred_deploy), display_labels = ['Normal', 'Fraude'])\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "plt.grid(False)\n",
    "cmp1.plot(ax = ax)\n",
    "plt.rc('font', **font)\n",
    "plt.title('Matriz de confusão - Pickle Modelo Final (Simulação Deploy)', fontsize = 15)\n",
    "plt.show()\n",
    "\n",
    "print('* Porcentagens da matriz de confusão:\\n')\n",
    "final_cm = (100 * confusion_matrix(y_all, y_pred_deploy, normalize='all')).astype('str')\n",
    "for n, i in enumerate(final_cm):\n",
    "    final_cm[n] = i.astype('float').round(3)\n",
    "    \n",
    "print(final_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['isFraud'].value_counts(normalize=True)*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a81448d8252054de048cf8568e1635fe80f1d657e07de5b5f4956e5ea2068263"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
